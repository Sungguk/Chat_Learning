{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224d Deep Learning for Natural Language Processing\n",
    "## Lecture 2: Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we represent the meaning of a word?\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"image/2.png\">\n",
    "\n",
    "\n",
    "\n",
    "### webster dictionary ? \n",
    "- Webster-dictionary.org is a free online English dictionary and thesaurus with more than 240,000 words and definitions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to represent meaning in a computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Word 를 Tree구조로 표현하여 관계를 표현\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/wordnet-hierarchy.png\">\n",
    "\n",
    "\n",
    "\n",
    "# 1. Word를 Tree구조를 이용하여 상위어, 동의어 등 관련 단어들을 표현하는 방법.\n",
    "\n",
    "- hypernyms : 상위어\n",
    "- Hyponyms : 하위어\n",
    "- synonym : 동의어\n",
    "\n",
    "wordNet이란 Database Content\n",
    "2006년 데이터 베이스에는 11만 5천개의 synsets(동의어 집합)으로 구성된 단어가 \n",
    "총 20만 7천개의 word-sense pairs(단어-의미 쌍)이 저장됨. 압축된 용량은 12MB정도.\n",
    "wordNet은 nouns(명사), verbs(동사), adjectives(형용사), adverbs(부사)를 서로 구분하는데, \n",
    "이는 이들이 서로 다른 문법 규칙을 따르기 때문임.\n",
    "\n",
    "taxonomy(분류)를 이용해서 상위어와 동의어를 분류했다.\n",
    "\n",
    "모든 synset(동의어 집합)은 단어의 synonymous(동의어) 그룹 혹은 collocations(언어관계)를 포함한다.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"image/3.png\">\n",
    "\n",
    "\n",
    "\n",
    "### 그러나! 이러한 방법은\n",
    "### Discrete representation에 의한문제점들이있고, 뉘앙스를놓치기쉬우며 새로운단어들에취약하다.\n",
    "### 또한, 주관적이며 구축하는데 엄청난 노가다를 요구한다.\n",
    "\n",
    "참고) http://m.blog.daum.net/se9tember/2264885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. 그래서 다른 방법으로, 우리가 알고 있는 one-hot-Vector 방식으로 표현해 보자.\n",
    "\n",
    "<img src=\"image/4.png\">\n",
    "\n",
    "### 그러나, 이 방법 또한 Sparse representation에 의해 단어의 수에 따라 차원 수가 엄청나게 증가하고, \n",
    "### 또한, 여전히 단어가 본질적으로 다른 단어와 어떻게 다른가를 알지 못한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 그렇다면, 그 단어가 수반하는 Context(문맥)을 이용해서 이웃들(neighbors)을 파악해보자!\n",
    "### 어떻게? Co-occurence matrix X를 이용해 보자 이 matrix는 그 단어와 함께 등장하는 단어들을 카운트하는 매트릭스이다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"image/5.png\">\n",
    "\n",
    "\n",
    "\n",
    "### 하지만... 이것 역시 Vocab size가 커질 수록 저장 공간이 많이 필요하게 되고 차원 수도 엄청나게 커진다. (Sparcity)\n",
    "## --> Models are less robust (건강하지 않은 모델!)\n",
    "\n",
    "### Solution: Low dimensional vectors \n",
    "\n",
    "### 가장 핵심적인 정보들을 작고 고정된 차원의 Vector 안에 저장 하자\n",
    "#### -> A dense vector usually around 25-1000 dimensions\n",
    "\n",
    "### How to reduce the dimensionality?\n",
    "\n",
    "<img src=\"image/6.png\">\n",
    "\n",
    "<img src=\"image/svd.png\">\n",
    "\n",
    "### SVD : 특이값 분해 (선형대수의 꽃)\n",
    "\n",
    "- 특이값 분해(SVD)는 고유값 분해(eigendecomposition)처럼 행렬을 대각화하는 한 방법이다. \n",
    "- 그런데, 특이값 분해가 유용한 이유는 행렬이 정방행렬이든 아니든 관계없이 모든 m x n 행렬에 대해 적용 가능하기 때문이다\n",
    "- 이 방법은 Semantic pattern을 발견할 수 있으나, 계산 비용이 n^3만큼 높다.\n",
    "\n",
    "\n",
    "-https://ko.wikipedia.org/wiki/%ED%8A%B9%EC%9D%B4%EA%B0%92_%EB%B6%84%ED%95%B4\n",
    "\n",
    "\n",
    "- http://darkpgmr.tistory.com/106\n",
    "\n",
    "\n",
    "- http://rfriend.tistory.com/185\n",
    "\n",
    "\n",
    "- 차원축소 비교 : http://www.slideshare.net/madvirus/pca-svd\n",
    "\n",
    "\n",
    "- 참고\n",
    "\n",
    "<img src=\"image/simi-func.png\">\n",
    "\n",
    "<img src=\"image/simi-diff.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word2Vec\n",
    "\n",
    "### 그럼 처음부터 낮은 차원의 word vecrors를 학습하자. \n",
    "\n",
    "- Word2Vec는 구글의 연구원인 Tomas Mikolov와 Kai Chen, Greg Corrado, Jeffrey Dean에 의해 쓰여진 논문인 \"Efficient Estimation of Word Representations in Vector Space(백터공간상에서 단어 의미의 효율적인 추정)\"에서 제안된 방법을 구현한 알고리즘으로 기존의 알고리즘에 비해 자연언어 처리 분야에서 비약적인 정밀도 향상을 가능하게 하고 있다.\n",
    "\n",
    "<img src=\"image/7.png\">\n",
    "\n",
    "- neural probabilistic language model ( Bengioet al, 2003)\n",
    "    - http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    - Language Model에 distributional representation을 적용한 대표 논문\n",
    "    - 계산복잡도 높음\n",
    "    \n",
    "    \n",
    "- NLP (almost) from scratch(Collobert& Weston, 2008)\n",
    "    - http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\n",
    "    \n",
    "\n",
    "- <font color=\"red\">Efficient Estimation of Word Representation in Vector space(Mikolovet al, 2013)</font>\n",
    "    - https://arxiv.org/pdf/1301.3781.pdf\n",
    "    - word2vec 방법론 초기 논문\n",
    "    - 준수한 성능에 계산복잡도 감소\n",
    "    - CBOW, Skip-gram\n",
    "    \n",
    "참고 \n",
    "- 논문 정리 잘된 블로그 : http://nohhj.blogspot.kr/2015/08/word-embedding.html\n",
    "\n",
    "\n",
    "- http://www.slideshare.net/JungkyuLee1/from-a-neural-probalistic-language-model-to-word2vec\n",
    "\n",
    "\n",
    "- LSI : Latent semantic indexing \n",
    "    - http://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html\n",
    "\n",
    "## Word2Vec의 메인 아이디어\n",
    "- Co-occurrence를 사용하지 말고, Predict surrounding words of every word\n",
    "- (주변 단어들로 부터 예측을 하자)\n",
    "\n",
    "<img src=\"image/8.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of Word2Vec\n",
    "\n",
    "<img src=\"image/9.png\">\n",
    "\n",
    "<img src=\"image/10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/Objective functions\n",
    "\n",
    "<img src=\"image/11.png\">\n",
    "\n",
    "<img src=\"image/12.png\">\n",
    "\n",
    "\n",
    "### Log를취해다더한다. (for Maximum Likelihood)\n",
    "### 그리고 이러한 과정을 모든 윈도우(트레이닝셋)만큼 거치고나면 Cost Function이된다.\n",
    "### 이를 Gradient Descent로최적화한다. (수식유도생략.. 동영상참고바람)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based vs direct prediction\n",
    "\n",
    "<img src=\"image/13.png\">\n",
    "\n",
    "\n",
    "- 참고\n",
    "\n",
    "<img src=\"image/NNLM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "참고\n",
    "\n",
    "- http://www.moreagile.net/2014/11/word2vec.html\n",
    "\n",
    "\n",
    "- 추천시스템이 Word2Vec을 만나면 : http://www.slideshare.net/ssuser2fe594/2015-py-con-word2vec\n",
    "\n",
    "\n",
    "- From A Neural Probalistic Language Model to Word2vec : http://www.slideshare.net/JungkyuLee1/from-a-neural-probalistic-language-model-to-word2vec\n",
    "\n",
    "\n",
    "- Word2Vec Cost Function : https://stackoverflow.com/questions/38478776/cost-function-for-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
