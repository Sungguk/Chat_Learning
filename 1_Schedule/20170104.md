<< 20170104 / Wed>>

1. CS224d Lecture 4 : Word Window Classification and Neural Networks [백병인님]
  - 다시 Deep Learning의 기초가 시작됨.
  - 끝에 학생들이 한 프로젝트가 보석
  - Loss function - Cross Entropy함수에 대해 수학적으로 이해시키려고 함.

  - 지도학습을 수학적으로 풀어보자!
  Classification
  logistic regression
  cross Entropy
  logistic으로 바라본 딥러닝, 자연어, 윈도우
  그러나, logistic은 아님 딥러닝을 쓰자!
  logistic - 딥러닝 관련있다
  백프로파게이션 다음시간에~
  오늘은, 딥러닝 에듀반 첫 시간인거 같음 ㅎㅎ
  - Cross Entropy에 대한 자세한 설명!
  http://blog.acronym.co.kr/433

  linear boundary는

  regularlization : 모델의 수를 떨쳐서 평균화, 오버피팅을 막기 위해 일부러 데이터를 훼손 시키는 것
  - L2 regularlization
  normalization : 분포를 정규화

2. Glove 논문 - 강은숙
  - Glove To Vec은 Word2Vec에 비해 Co-Occurence Matrix를 행렬 인수분해 방법을 통해 0이 아닌 요소만 학습 시킴으로써 속도가 빠르다.
  - 하지만, 일정 시점이 넘어가는 면에 있어서는 (논문에서 xmax=100으로 설정한 그래프) 100 이상이면 100으로 퉁 설정하기 때문에
  계속해서 Linear하게 빈도수를 체크하는 Word2Vec에 비해 단점이 존재한다.

3.  Glove Source - 전수진
  - Python을 이용해서 build Co-Occurence Matrix를 구축하는 소스 설명
  - window Size를 설정해 놓고, Vocabulary Size만큼 Iteration을 돌리면서
  Search 하는 word를 기준으로 주변 단어들 간의 distance의 값을 1/distance 의 값을 표에 채운다.
  - 남은 소스에 대해서는 다음시간에 계속~
