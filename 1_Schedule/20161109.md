
<< 11/9 Wen >>

1. NLTK : 은숙님
  - 소개
  - 사용법
  - 제공하는 기능들
  - 다음 주에 예제 돌려본 것 공유!
http://www.nltk.org/
https://www.lucypark.kr/courses/2015-dm/text-mining.html
https://www.datascienceschool.net/view-notebook/118731eec74b4ad3bdd2f89bab077e1b/

2. Word2Vec : 창욱님
  - python join함수 : 문장을 join하고 space로 spilt
  - tf.nn.nce_loss : softmax와 비슷, 코사인 유사도 대신 사용 , 좀더 알아보기!
  - tf.nn.embedding_lookup 좀더 알아보기!
  - cbow : context > word
  - skip-gram : word > context
  - 배경 지식 : tf, if
https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/tutorials/word2vec/word2vec_basic.py
http://khanrc.tistory.com/entry/TensorFlow-7-word2vec-Implementation

3. 성진님
  - 최규민님 발표자료 공유
  - 아프리카TV, 추천 서비스 하고있음.
http://www.slideshare.net/ssuser2fe594/2015-py-con-word2vec
  - 넷플릭스 : 코사인 유사도
  - 왓챠 : 사용자 별점대로 군집화
http://www.slideshare.net/ssuser2fe594/deview2014-live-broadcasting

4. 논문 - 기호님
  - Sequence to Sequence Learning for NLP
  https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html
  - Greedy : 탐욕적으로 가장 최대 좋은것만 취함, Multi Armed Bandit Algorithm
  - beam search : 여러가지 주어지는 거
  - AutoReply
  - SmartReplay
  - embedding 내부 알고리즘 원리! 알아야함!
  - 다음주에 Seq to Seq 코드리뷰 발표
  - word2vec에 입력값을 넣으면 뭐가 나올까?

5. Coursera : 성진님
  - 강의 설명

6. Service 회의

<< 다음주 >>
  1. 기호님 : Seq to Seq 나머지, 소스코드 - 1시간
  2. 광재님 : 서비스 - 30분
  3. 성진님 : Coursera - 30분
  4. 은숙님: NLTK 마무리 - 25분

<< Question >>
1. Word로 학습을 시키면 잘못된 문장이 나올 수도 있으니, 문장 단위로 학습을 시켜서 하는게 좋지 않을까?
    - 문장 자체를 하나의 단어 처럼 학습을 하게 되면, 중복해서 3문장들을 사용하지 않지 않나
    - Sentence to Vector
    - Doc to Vector : 논문, report
    - Semantic Similarlity

<< NLP의 대표적인 예들 >>
    - 번역
    - summary : 장문의 영어지문을 요약해줌
    - word2vec 상용 사례 : 뉴욕타임즈에서 음식 레시피 만들어줌, 문장을 기고해줌
    - IBM : chief wathson, 재료의 화학적 성분까지 파악해서 음식 추천해줌 ex) 김치와 어울리는 음식
    - 강화학습 챗봇 문장 생성 : 유명작가의 소설들을 학습 시켜서, 문장을 생성 시킴

<< 기타 >>
  - Text가 이미지보다 시간이 오래걸림
  - factorial 연산하게 될것임!
  - 원리 이해 Application
