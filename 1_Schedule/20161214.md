<< 12/14 Wed >>

1. 전창욱님 - simpleDS 두 번째 발표
- 강화학습을 기본으로 학습시키는 방법론
- 나이브 베이지 방법으로 함.
- 마스터 알고리즘 책 소개, 연결주의 나이브 베이즈

- 아마존 에코, api.api : 사용자가 질문한 내역을 다시한번 되 물어본다
- 일단 소스는 keep 해 두고 기본기를 익힌 다음 활용할 부분이 생기면 활용하자.

2. 신성진님 - Coursera 6~7장
- UX : 수치화 시킨다. : 추후에 공유 예정
1) Probabilites
  - 기본 통계 확률 방법론
2) Bayes Theorem
1종오류 : 참이 아닌데 거짓이라 하는거
2종오류 : 참인데 참이라고 안하는거
검정력

3) Language Modeling
n-gram
HMM : 자세히 뭔지? 다음주에 소개!
확률이 0일때, 라플라스 모델링을 이용해서 1로 만들어 버림. 모든 경우의 수에 다 더해서 확률값이 나올 수 있게함

3. 강은숙 - CS224d Lecture2 Word vector
- Word2Vec 이 나오기 이전에 컴퓨터가 이해 할 수 있게 언어를 모델링 하는 방법들이 존재함.
1) Word tree
2) one-hot-Vector
3) Co-occurence matrix
4) Low dimension 으로 처음부터 학습
  - SVD(특이값 분해) : 차원 축소를 하기 위해서 벡터의 고유값을 추출하여 벡터(단어)간의 관계를 찾아내어 차원을 줄임.
- Main Idea : Word2Vec!
  - Word2Vec 으로 가기 전에 논문들이 존재함.
  - CBOW 보단 Skip-gram 이 성능이 더 좋음.
  - skip-gram 의 object function은 중심단어 w t가 주어졌을때, 그 주변 단어 w t+j가 나올 확률을 log를 취해(Maximum likelihood방법을 위해)
    Sum한 후 시간 T까지 Sum 한후 1/T로 나눔
  - Softmax function으로 확률을 구함.


-- Word2Vec에 대한 기본적인 Source도 살펴 보는 것이 좋겟다.
